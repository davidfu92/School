{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HM4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHCDh5dYas-z"
      },
      "source": [
        "# Home 4: Build a CNN for image recognition.\n",
        "\n",
        "### Name: David Fu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFjzh4UHas-0"
      },
      "source": [
        "## 0. You will do the following:\n",
        "\n",
        "1. Read, complete, and run the code.\n",
        "\n",
        "2. **Make substantial improvements** to maximize the accurcy.\n",
        "    \n",
        "3. Convert the .IPYNB file to .HTML file.\n",
        "\n",
        "    * The HTML file must contain the code and the output after execution.\n",
        "    \n",
        "    * Missing **the output after execution** will not be graded.\n",
        "    \n",
        "4. Upload this .HTML file to your Google Drive, Dropbox, or Github repo. (If you submit the file to Google Drive or Dropbox, you must make the file \"open-access\". The delay caused by \"deny of access\" may result in late penalty.)\n",
        "\n",
        "4. Submit the link to this .HTML file to Canvas.\n",
        "\n",
        "    * Example: https://github.com/wangshusen/CS583-2020S/blob/master/homework/HM4/HM4.html\n",
        "\n",
        "\n",
        "## Requirements:\n",
        "\n",
        "1. You can use whatever CNN architecture, including VGG, Inception, and ResNet. However, you must build the networks layer by layer. You must NOT import the archetectures from ```keras.applications```.\n",
        "\n",
        "2. Make sure ```BatchNormalization``` is between a ```Conv```/```Dense``` layer and an ```activation``` layer.\n",
        "\n",
        "3. If you want to regularize a ```Conv```/```Dense``` layer, you should place a ```Dropout``` layer **before** the ```Conv```/```Dense``` layer.\n",
        "\n",
        "4. An accuracy above 70% is considered reasonable. An accuracy above 80% is considered good. Without data augmentation, achieving 80% accuracy is difficult.\n",
        "\n",
        "\n",
        "## Google Colab\n",
        "\n",
        "- If you do not have GPU, the training of a CNN can be slow. Google Colab is a good option.\n",
        "\n",
        "- Keep in mind that you must download it as an IPYNB file and then use IPython Notebook to convert it to HTML.\n",
        "\n",
        "- Also keep in mind that the IPYNB and HTML files must contain the outputs. (Otherwise, the instructor will not be able to know the correctness and performance.) Do the followings to keep the outputs.\n",
        "\n",
        "- In Colab, go to ```Runtime``` --> ```Change runtime type``` --> Do NOT check ```Omit code cell output when saving this notebook```. In this way, the downloaded IPYNB file contains the outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nEPXjwvas-1"
      },
      "source": [
        "## 1. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYUsOhzVas-2"
      },
      "source": [
        "### 1.1. Load data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc8ijfqFas-2",
        "outputId": "a9ff8f51-9802-4410-9128-99c1efbfa063"
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('shape of x_train: ' + str(x_train.shape))\n",
        "print('shape of y_train: ' + str(y_train.shape))\n",
        "print('shape of x_test: ' + str(x_test.shape))\n",
        "print('shape of y_test: ' + str(y_test.shape))\n",
        "print('number of classes: ' + str(np.max(y_train) - np.min(y_train) + 1))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of x_train: (50000, 32, 32, 3)\n",
            "shape of y_train: (50000, 1)\n",
            "shape of x_test: (10000, 32, 32, 3)\n",
            "shape of y_test: (10000, 1)\n",
            "number of classes: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRkW1nzYas-2"
      },
      "source": [
        "### 1.2. One-hot encode the labels\n",
        "\n",
        "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
        "\n",
        "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
        "\n",
        "2. Apply the function to ```y_train``` and ```y_test```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYW2zR4vas-3",
        "outputId": "f76f2b29-ef0d-47aa-80d8-e03450f66f1d"
      },
      "source": [
        "def to_one_hot(y, num_class=10):\n",
        "    #print(y)\n",
        "    encoded_y = np.zeros([(len(y)), num_class])\n",
        "    for v in enumerate(y):\n",
        "      encoded_y[v] = 1\n",
        "    return encoded_y\n",
        "\n",
        "y_train_vec = to_one_hot(y_train)\n",
        "y_test_vec = to_one_hot(y_test)\n",
        "\n",
        "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
        "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
        "\n",
        "print(y_train[0])\n",
        "print(y_train_vec[0])\n",
        "\n",
        "print(y_train[678])\n",
        "print(y_train_vec[678])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of y_train_vec: (50000, 10)\n",
            "Shape of y_test_vec: (10000, 10)\n",
            "[6]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "[3]\n",
            "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KXIPmH7as-3"
      },
      "source": [
        "#### Remark: the outputs should be\n",
        "* Shape of y_train_vec: (50000, 10)\n",
        "* Shape of y_test_vec: (10000, 10)\n",
        "* [6]\n",
        "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq5dbok0as-3"
      },
      "source": [
        "### 1.3. Randomly partition the training set to training and validation sets\n",
        "\n",
        "Randomly partition the 50K training samples to 2 sets:\n",
        "* a training set containing 40K samples\n",
        "* a validation set containing 10K samples\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbRuPKu6as-3",
        "outputId": "2c1ec274-646a-4682-a3db-e37139210c66"
      },
      "source": [
        "rand_indices = np.random.permutation(50000)\n",
        "train_indices = rand_indices[0:40000]\n",
        "valid_indices = rand_indices[40000:50000]\n",
        "\n",
        "x_val = x_train[valid_indices, :]\n",
        "y_val = y_train_vec[valid_indices, :]\n",
        "\n",
        "x_tr = x_train[train_indices, :]\n",
        "y_tr = y_train_vec[train_indices, :]\n",
        "\n",
        "print('Shape of x_tr: ' + str(x_tr.shape))\n",
        "print('Shape of y_tr: ' + str(y_tr.shape))\n",
        "print('Shape of x_val: ' + str(x_val.shape))\n",
        "print('Shape of y_val: ' + str(y_val.shape))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of x_tr: (40000, 32, 32, 3)\n",
            "Shape of y_tr: (40000, 10)\n",
            "Shape of x_val: (10000, 32, 32, 3)\n",
            "Shape of y_val: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgwVB9r7as-4"
      },
      "source": [
        "## 2. Build a CNN and tune its hyper-parameters\n",
        "\n",
        "1. Build a convolutional neural network model\n",
        "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
        "    * Do NOT use test data for hyper-parameter tuning!!!\n",
        "3. Try to achieve a validation accuracy as high as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8Ig2uHDas-4"
      },
      "source": [
        "### Remark: \n",
        "\n",
        "The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n",
        "* Add more layers.\n",
        "* Use regularizations, e.g., dropout.\n",
        "* Use batch normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0MF_5nZas-4",
        "outputId": "1179fdb3-afb5-4f43-a232-5e45d5d44db6"
      },
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation, Dropout, LeakyReLU\n",
        "from keras.models import Sequential\n",
        "from keras.applications import ResNet50, InceptionV3\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
        "# model.add(LeakyReLU(alpha=0.01))\n",
        "model.add(MaxPooling2D((2, 2), padding='same'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2), padding='same'))\n",
        "model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling2D((2, 2), padding='same'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv2D(256, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling2D((2, 2), padding='same'))\n",
        "model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "# model.add(LeakyReLU(alpha=0.01))  \n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3)))\n",
        "# model.add(Conv2D(16, (3, 3), padding='same'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Activation(\"relu\"))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Conv2D(32, (3, 3), padding='same'))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Activation(\"sigmoid\"))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "# model.add(Activation(\"tanh\"))\n",
        "# model.add(Conv2D(16, (3, 3), padding='same'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(LeakyReLU())\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Dropout(0.4))\n",
        "# model.add(Conv2D(32, (3, 3), padding='same'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Activation(\"relu\"))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(512, activation='relu'))\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 4, 4, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 4, 4, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 2, 2, 128)         295040    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 1, 1, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 850,890\n",
            "Trainable params: 849,866\n",
            "Non-trainable params: 1,024\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liDp4xnOOE7n"
      },
      "source": [
        "# model = InceptionV3(include_top=True,\n",
        "#     weights=\"imagenet\",\n",
        "#     input_tensor=None,\n",
        "#     input_shape=None,\n",
        "#     pooling=None,\n",
        "#     classes=1000)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hreXyCU9as-4"
      },
      "source": [
        "from keras import optimizers\n",
        "\n",
        "learning_rate = 1E-3 # to be tuned!\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.Adam(lr=learning_rate),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niuvW0gsas-4",
        "outputId": "490678a6-5a37-48ac-ddbe-cb6adf25ac59"
      },
      "source": [
        "history = model.fit(x_tr, y_tr, batch_size=32, epochs=50, validation_data=(x_val, y_val))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1250/1250 [==============================] - 11s 6ms/step - loss: 1.9203 - acc: 0.3021 - val_loss: 1.3646 - val_acc: 0.5014\n",
            "Epoch 2/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.2681 - acc: 0.5456 - val_loss: 1.1397 - val_acc: 0.5819\n",
            "Epoch 3/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.0719 - acc: 0.6172 - val_loss: 1.1874 - val_acc: 0.5751\n",
            "Epoch 4/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9506 - acc: 0.6637 - val_loss: 0.9498 - val_acc: 0.6679\n",
            "Epoch 5/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8712 - acc: 0.6946 - val_loss: 0.7757 - val_acc: 0.7217\n",
            "Epoch 6/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7893 - acc: 0.7262 - val_loss: 0.9199 - val_acc: 0.6818\n",
            "Epoch 7/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7536 - acc: 0.7373 - val_loss: 0.7295 - val_acc: 0.7449\n",
            "Epoch 8/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7010 - acc: 0.7580 - val_loss: 0.7501 - val_acc: 0.7334\n",
            "Epoch 9/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.6521 - acc: 0.7730 - val_loss: 0.7891 - val_acc: 0.7295\n",
            "Epoch 10/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.6249 - acc: 0.7821 - val_loss: 0.6631 - val_acc: 0.7752\n",
            "Epoch 11/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.5974 - acc: 0.7907 - val_loss: 0.7956 - val_acc: 0.7245\n",
            "Epoch 12/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.5724 - acc: 0.8053 - val_loss: 0.7120 - val_acc: 0.7550\n",
            "Epoch 13/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.5498 - acc: 0.8111 - val_loss: 0.6043 - val_acc: 0.7895\n",
            "Epoch 14/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.5265 - acc: 0.8201 - val_loss: 0.6534 - val_acc: 0.7770\n",
            "Epoch 15/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.5049 - acc: 0.8251 - val_loss: 0.6179 - val_acc: 0.7837\n",
            "Epoch 16/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.4921 - acc: 0.8299 - val_loss: 0.7189 - val_acc: 0.7525\n",
            "Epoch 17/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.4790 - acc: 0.8347 - val_loss: 0.6267 - val_acc: 0.7901\n",
            "Epoch 18/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.4501 - acc: 0.8436 - val_loss: 0.6159 - val_acc: 0.7914\n",
            "Epoch 19/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.4454 - acc: 0.8460 - val_loss: 0.6496 - val_acc: 0.7782\n",
            "Epoch 20/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.4232 - acc: 0.8516 - val_loss: 0.6524 - val_acc: 0.7821\n",
            "Epoch 21/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.4168 - acc: 0.8566 - val_loss: 0.5946 - val_acc: 0.8060\n",
            "Epoch 22/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.4059 - acc: 0.8589 - val_loss: 0.6053 - val_acc: 0.7934\n",
            "Epoch 23/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.4098 - acc: 0.8581 - val_loss: 0.6073 - val_acc: 0.7942\n",
            "Epoch 24/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3881 - acc: 0.8653 - val_loss: 0.5601 - val_acc: 0.8120\n",
            "Epoch 25/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3774 - acc: 0.8694 - val_loss: 0.5721 - val_acc: 0.8035\n",
            "Epoch 26/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3641 - acc: 0.8757 - val_loss: 0.5855 - val_acc: 0.8021\n",
            "Epoch 27/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3641 - acc: 0.8714 - val_loss: 0.6130 - val_acc: 0.7944\n",
            "Epoch 28/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3595 - acc: 0.8767 - val_loss: 0.6341 - val_acc: 0.7939\n",
            "Epoch 29/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3499 - acc: 0.8773 - val_loss: 0.5624 - val_acc: 0.8116\n",
            "Epoch 30/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3362 - acc: 0.8811 - val_loss: 0.6286 - val_acc: 0.8001\n",
            "Epoch 31/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3226 - acc: 0.8887 - val_loss: 0.7480 - val_acc: 0.7624\n",
            "Epoch 32/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3274 - acc: 0.8862 - val_loss: 0.5912 - val_acc: 0.8096\n",
            "Epoch 33/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3149 - acc: 0.8903 - val_loss: 0.6256 - val_acc: 0.7905\n",
            "Epoch 34/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3103 - acc: 0.8919 - val_loss: 0.6026 - val_acc: 0.8055\n",
            "Epoch 35/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2972 - acc: 0.8960 - val_loss: 0.5887 - val_acc: 0.8101\n",
            "Epoch 36/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3026 - acc: 0.8954 - val_loss: 0.5707 - val_acc: 0.8125\n",
            "Epoch 37/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2962 - acc: 0.8963 - val_loss: 0.6198 - val_acc: 0.8088\n",
            "Epoch 38/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2936 - acc: 0.8997 - val_loss: 0.6176 - val_acc: 0.8042\n",
            "Epoch 39/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2899 - acc: 0.8982 - val_loss: 0.5721 - val_acc: 0.8130\n",
            "Epoch 40/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2730 - acc: 0.9043 - val_loss: 0.5981 - val_acc: 0.8070\n",
            "Epoch 41/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2700 - acc: 0.9041 - val_loss: 0.5787 - val_acc: 0.8089\n",
            "Epoch 42/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2642 - acc: 0.9083 - val_loss: 0.5838 - val_acc: 0.8095\n",
            "Epoch 43/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2690 - acc: 0.9051 - val_loss: 0.5903 - val_acc: 0.8109\n",
            "Epoch 44/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2638 - acc: 0.9085 - val_loss: 0.5837 - val_acc: 0.8128\n",
            "Epoch 45/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2607 - acc: 0.9106 - val_loss: 0.6011 - val_acc: 0.8110\n",
            "Epoch 46/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2487 - acc: 0.9143 - val_loss: 0.5780 - val_acc: 0.8142\n",
            "Epoch 47/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2442 - acc: 0.9157 - val_loss: 0.6087 - val_acc: 0.8051\n",
            "Epoch 48/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2515 - acc: 0.9122 - val_loss: 0.6382 - val_acc: 0.8011\n",
            "Epoch 49/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2341 - acc: 0.9179 - val_loss: 0.6943 - val_acc: 0.7993\n",
            "Epoch 50/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.2377 - acc: 0.9162 - val_loss: 0.6257 - val_acc: 0.8123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "Cxp-dyoGas-5",
        "outputId": "502a2626-34b1-421d-de58-2b5e463e73fa"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2REyY2Xdas-5"
      },
      "source": [
        "## 3. Train (again) and evaluate the model\n",
        "\n",
        "- To this end, you have found the \"best\" hyper-parameters. \n",
        "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
        "- Evaluate your model on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MFf1El6as-5"
      },
      "source": [
        "### 3.1. Train the model on the entire training set\n",
        "\n",
        "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN7_uLiGas-5"
      },
      "source": [
        "learning_rate = 2.5E-4 # to be tuned!\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.Adam(lr=learning_rate),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr2eDyVfas-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bcb24fb-334a-4899-e8b8-de3dfac972de"
      },
      "source": [
        "history = model.fit(x_train, y_train_vec, batch_size=64, epochs=200, validation_data=(x_val, y_val))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.2938 - acc: 0.9073 - val_loss: 0.4710 - val_acc: 0.8433\n",
            "Epoch 2/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2684 - acc: 0.9146 - val_loss: 0.4416 - val_acc: 0.8504\n",
            "Epoch 3/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2578 - acc: 0.9173 - val_loss: 0.4022 - val_acc: 0.8626\n",
            "Epoch 4/200\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.2436 - acc: 0.9216 - val_loss: 0.3776 - val_acc: 0.8701\n",
            "Epoch 5/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2387 - acc: 0.9230 - val_loss: 0.3514 - val_acc: 0.8778\n",
            "Epoch 6/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2314 - acc: 0.9256 - val_loss: 0.3328 - val_acc: 0.8816\n",
            "Epoch 7/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2257 - acc: 0.9273 - val_loss: 0.3139 - val_acc: 0.8912\n",
            "Epoch 8/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2169 - acc: 0.9291 - val_loss: 0.2947 - val_acc: 0.8976\n",
            "Epoch 9/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2144 - acc: 0.9312 - val_loss: 0.2719 - val_acc: 0.9069\n",
            "Epoch 10/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2006 - acc: 0.9343 - val_loss: 0.2629 - val_acc: 0.9098\n",
            "Epoch 11/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2076 - acc: 0.9325 - val_loss: 0.2509 - val_acc: 0.9133\n",
            "Epoch 12/200\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.1988 - acc: 0.9347 - val_loss: 0.2338 - val_acc: 0.9211\n",
            "Epoch 13/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1963 - acc: 0.9342 - val_loss: 0.2193 - val_acc: 0.9282\n",
            "Epoch 14/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1953 - acc: 0.9358 - val_loss: 0.2171 - val_acc: 0.9282\n",
            "Epoch 15/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1902 - acc: 0.9367 - val_loss: 0.1995 - val_acc: 0.9346\n",
            "Epoch 16/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1838 - acc: 0.9383 - val_loss: 0.1885 - val_acc: 0.9386\n",
            "Epoch 17/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1760 - acc: 0.9406 - val_loss: 0.1853 - val_acc: 0.9410\n",
            "Epoch 18/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1728 - acc: 0.9418 - val_loss: 0.1671 - val_acc: 0.9456\n",
            "Epoch 19/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1771 - acc: 0.9416 - val_loss: 0.1587 - val_acc: 0.9498\n",
            "Epoch 20/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1708 - acc: 0.9413 - val_loss: 0.1526 - val_acc: 0.9525\n",
            "Epoch 21/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1686 - acc: 0.9434 - val_loss: 0.1387 - val_acc: 0.9576\n",
            "Epoch 22/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1675 - acc: 0.9424 - val_loss: 0.1518 - val_acc: 0.9507\n",
            "Epoch 23/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1651 - acc: 0.9436 - val_loss: 0.1384 - val_acc: 0.9581\n",
            "Epoch 24/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1633 - acc: 0.9431 - val_loss: 0.1198 - val_acc: 0.9651\n",
            "Epoch 25/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1613 - acc: 0.9463 - val_loss: 0.1224 - val_acc: 0.9637\n",
            "Epoch 26/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1557 - acc: 0.9481 - val_loss: 0.1083 - val_acc: 0.9702\n",
            "Epoch 27/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1526 - acc: 0.9469 - val_loss: 0.1038 - val_acc: 0.9712\n",
            "Epoch 28/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1483 - acc: 0.9503 - val_loss: 0.0982 - val_acc: 0.9736\n",
            "Epoch 29/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1560 - acc: 0.9463 - val_loss: 0.0942 - val_acc: 0.9734\n",
            "Epoch 30/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1532 - acc: 0.9472 - val_loss: 0.0962 - val_acc: 0.9724\n",
            "Epoch 31/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1518 - acc: 0.9460 - val_loss: 0.0881 - val_acc: 0.9756\n",
            "Epoch 32/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1468 - acc: 0.9491 - val_loss: 0.0830 - val_acc: 0.9764\n",
            "Epoch 33/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1452 - acc: 0.9499 - val_loss: 0.0773 - val_acc: 0.9785\n",
            "Epoch 34/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1481 - acc: 0.9482 - val_loss: 0.0749 - val_acc: 0.9797\n",
            "Epoch 35/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1392 - acc: 0.9527 - val_loss: 0.0735 - val_acc: 0.9812\n",
            "Epoch 36/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1395 - acc: 0.9530 - val_loss: 0.0706 - val_acc: 0.9809\n",
            "Epoch 37/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1360 - acc: 0.9521 - val_loss: 0.0667 - val_acc: 0.9815\n",
            "Epoch 38/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1299 - acc: 0.9565 - val_loss: 0.0598 - val_acc: 0.9854\n",
            "Epoch 39/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1351 - acc: 0.9538 - val_loss: 0.0655 - val_acc: 0.9827\n",
            "Epoch 40/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1306 - acc: 0.9554 - val_loss: 0.0589 - val_acc: 0.9850\n",
            "Epoch 41/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1314 - acc: 0.9535 - val_loss: 0.0633 - val_acc: 0.9827\n",
            "Epoch 42/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1303 - acc: 0.9547 - val_loss: 0.0544 - val_acc: 0.9857\n",
            "Epoch 43/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1288 - acc: 0.9544 - val_loss: 0.0508 - val_acc: 0.9867\n",
            "Epoch 44/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1276 - acc: 0.9560 - val_loss: 0.0469 - val_acc: 0.9880\n",
            "Epoch 45/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1266 - acc: 0.9557 - val_loss: 0.0550 - val_acc: 0.9874\n",
            "Epoch 46/200\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.1228 - acc: 0.9589 - val_loss: 0.0470 - val_acc: 0.9885\n",
            "Epoch 47/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1213 - acc: 0.9584 - val_loss: 0.0415 - val_acc: 0.9904\n",
            "Epoch 48/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1222 - acc: 0.9570 - val_loss: 0.0395 - val_acc: 0.9915\n",
            "Epoch 49/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1229 - acc: 0.9575 - val_loss: 0.0415 - val_acc: 0.9911\n",
            "Epoch 50/200\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.1164 - acc: 0.9597 - val_loss: 0.0364 - val_acc: 0.9927\n",
            "Epoch 51/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1187 - acc: 0.9580 - val_loss: 0.0426 - val_acc: 0.9907\n",
            "Epoch 52/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1212 - acc: 0.9574 - val_loss: 0.0368 - val_acc: 0.9926\n",
            "Epoch 53/200\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.1154 - acc: 0.9589 - val_loss: 0.0333 - val_acc: 0.9929\n",
            "Epoch 54/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1126 - acc: 0.9601 - val_loss: 0.0338 - val_acc: 0.9934\n",
            "Epoch 55/200\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.1181 - acc: 0.9595 - val_loss: 0.0327 - val_acc: 0.9936\n",
            "Epoch 56/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1165 - acc: 0.9585 - val_loss: 0.0326 - val_acc: 0.9939\n",
            "Epoch 57/200\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.1103 - acc: 0.9603 - val_loss: 0.0312 - val_acc: 0.9941\n",
            "Epoch 58/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1119 - acc: 0.9611 - val_loss: 0.0270 - val_acc: 0.9950\n",
            "Epoch 59/200\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.1126 - acc: 0.9612 - val_loss: 0.0268 - val_acc: 0.9943\n",
            "Epoch 60/200\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.1123 - acc: 0.9614 - val_loss: 0.0254 - val_acc: 0.9959\n",
            "Epoch 61/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1077 - acc: 0.9609 - val_loss: 0.0253 - val_acc: 0.9949\n",
            "Epoch 62/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1077 - acc: 0.9625 - val_loss: 0.0230 - val_acc: 0.9963\n",
            "Epoch 63/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1016 - acc: 0.9643 - val_loss: 0.0200 - val_acc: 0.9969\n",
            "Epoch 64/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1141 - acc: 0.9610 - val_loss: 0.0217 - val_acc: 0.9965\n",
            "Epoch 65/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1049 - acc: 0.9637 - val_loss: 0.0230 - val_acc: 0.9953\n",
            "Epoch 66/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1049 - acc: 0.9640 - val_loss: 0.0182 - val_acc: 0.9972\n",
            "Epoch 67/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1082 - acc: 0.9627 - val_loss: 0.0198 - val_acc: 0.9971\n",
            "Epoch 68/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1043 - acc: 0.9637 - val_loss: 0.0179 - val_acc: 0.9968\n",
            "Epoch 69/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1021 - acc: 0.9634 - val_loss: 0.0185 - val_acc: 0.9969\n",
            "Epoch 70/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1048 - acc: 0.9649 - val_loss: 0.0174 - val_acc: 0.9969\n",
            "Epoch 71/200\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.1011 - acc: 0.9651 - val_loss: 0.0201 - val_acc: 0.9964\n",
            "Epoch 72/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1022 - acc: 0.9652 - val_loss: 0.0158 - val_acc: 0.9976\n",
            "Epoch 73/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0966 - acc: 0.9664 - val_loss: 0.0167 - val_acc: 0.9970\n",
            "Epoch 74/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1004 - acc: 0.9647 - val_loss: 0.0176 - val_acc: 0.9971\n",
            "Epoch 75/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0947 - acc: 0.9669 - val_loss: 0.0149 - val_acc: 0.9981\n",
            "Epoch 76/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1003 - acc: 0.9654 - val_loss: 0.0155 - val_acc: 0.9977\n",
            "Epoch 77/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0979 - acc: 0.9667 - val_loss: 0.0162 - val_acc: 0.9976\n",
            "Epoch 78/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0983 - acc: 0.9654 - val_loss: 0.0160 - val_acc: 0.9979\n",
            "Epoch 79/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0969 - acc: 0.9663 - val_loss: 0.0126 - val_acc: 0.9986\n",
            "Epoch 80/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0949 - acc: 0.9674 - val_loss: 0.0124 - val_acc: 0.9983\n",
            "Epoch 81/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0953 - acc: 0.9659 - val_loss: 0.0137 - val_acc: 0.9980\n",
            "Epoch 82/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0918 - acc: 0.9671 - val_loss: 0.0105 - val_acc: 0.9988\n",
            "Epoch 83/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.1001 - acc: 0.9658 - val_loss: 0.0107 - val_acc: 0.9988\n",
            "Epoch 84/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0941 - acc: 0.9677 - val_loss: 0.0108 - val_acc: 0.9987\n",
            "Epoch 85/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0959 - acc: 0.9672 - val_loss: 0.0122 - val_acc: 0.9987\n",
            "Epoch 86/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0940 - acc: 0.9680 - val_loss: 0.0100 - val_acc: 0.9989\n",
            "Epoch 87/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0926 - acc: 0.9678 - val_loss: 0.0095 - val_acc: 0.9986\n",
            "Epoch 88/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0929 - acc: 0.9670 - val_loss: 0.0093 - val_acc: 0.9990\n",
            "Epoch 89/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0946 - acc: 0.9672 - val_loss: 0.0104 - val_acc: 0.9990\n",
            "Epoch 90/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0887 - acc: 0.9686 - val_loss: 0.0084 - val_acc: 0.9992\n",
            "Epoch 91/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0897 - acc: 0.9690 - val_loss: 0.0082 - val_acc: 0.9992\n",
            "Epoch 92/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0921 - acc: 0.9678 - val_loss: 0.0103 - val_acc: 0.9989\n",
            "Epoch 93/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0895 - acc: 0.9682 - val_loss: 0.0072 - val_acc: 0.9993\n",
            "Epoch 94/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0955 - acc: 0.9671 - val_loss: 0.0085 - val_acc: 0.9993\n",
            "Epoch 95/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0875 - acc: 0.9689 - val_loss: 0.0081 - val_acc: 0.9995\n",
            "Epoch 96/200\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0902 - acc: 0.9687 - val_loss: 0.0090 - val_acc: 0.9994\n",
            "Epoch 97/200\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0891 - acc: 0.9678 - val_loss: 0.0092 - val_acc: 0.9991\n",
            "Epoch 98/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0858 - acc: 0.9697 - val_loss: 0.0088 - val_acc: 0.9992\n",
            "Epoch 99/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0890 - acc: 0.9696 - val_loss: 0.0087 - val_acc: 0.9988\n",
            "Epoch 100/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0867 - acc: 0.9701 - val_loss: 0.0075 - val_acc: 0.9997\n",
            "Epoch 101/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0924 - acc: 0.9686 - val_loss: 0.0074 - val_acc: 0.9993\n",
            "Epoch 102/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0900 - acc: 0.9689 - val_loss: 0.0077 - val_acc: 0.9993\n",
            "Epoch 103/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0819 - acc: 0.9719 - val_loss: 0.0062 - val_acc: 0.9994\n",
            "Epoch 104/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0797 - acc: 0.9725 - val_loss: 0.0062 - val_acc: 0.9996\n",
            "Epoch 105/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0834 - acc: 0.9709 - val_loss: 0.0056 - val_acc: 0.9998\n",
            "Epoch 106/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0833 - acc: 0.9709 - val_loss: 0.0061 - val_acc: 0.9997\n",
            "Epoch 107/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0820 - acc: 0.9711 - val_loss: 0.0058 - val_acc: 0.9996\n",
            "Epoch 108/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0884 - acc: 0.9700 - val_loss: 0.0068 - val_acc: 0.9994\n",
            "Epoch 109/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0837 - acc: 0.9723 - val_loss: 0.0047 - val_acc: 0.9997\n",
            "Epoch 110/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0821 - acc: 0.9712 - val_loss: 0.0061 - val_acc: 0.9996\n",
            "Epoch 111/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0828 - acc: 0.9706 - val_loss: 0.0054 - val_acc: 0.9996\n",
            "Epoch 112/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0858 - acc: 0.9699 - val_loss: 0.0049 - val_acc: 0.9998\n",
            "Epoch 113/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0816 - acc: 0.9720 - val_loss: 0.0052 - val_acc: 0.9996\n",
            "Epoch 114/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0895 - acc: 0.9693 - val_loss: 0.0048 - val_acc: 0.9999\n",
            "Epoch 115/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0871 - acc: 0.9693 - val_loss: 0.0057 - val_acc: 0.9997\n",
            "Epoch 116/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0817 - acc: 0.9713 - val_loss: 0.0043 - val_acc: 0.9999\n",
            "Epoch 117/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0770 - acc: 0.9726 - val_loss: 0.0051 - val_acc: 0.9998\n",
            "Epoch 118/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0840 - acc: 0.9713 - val_loss: 0.0045 - val_acc: 0.9999\n",
            "Epoch 119/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0796 - acc: 0.9720 - val_loss: 0.0055 - val_acc: 0.9994\n",
            "Epoch 120/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0796 - acc: 0.9716 - val_loss: 0.0034 - val_acc: 0.9999\n",
            "Epoch 121/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0767 - acc: 0.9727 - val_loss: 0.0042 - val_acc: 0.9999\n",
            "Epoch 122/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0766 - acc: 0.9725 - val_loss: 0.0042 - val_acc: 0.9998\n",
            "Epoch 123/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0749 - acc: 0.9739 - val_loss: 0.0039 - val_acc: 0.9999\n",
            "Epoch 124/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0769 - acc: 0.9738 - val_loss: 0.0047 - val_acc: 1.0000\n",
            "Epoch 125/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0832 - acc: 0.9712 - val_loss: 0.0047 - val_acc: 0.9999\n",
            "Epoch 126/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0813 - acc: 0.9720 - val_loss: 0.0042 - val_acc: 0.9998\n",
            "Epoch 127/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0777 - acc: 0.9732 - val_loss: 0.0042 - val_acc: 0.9998\n",
            "Epoch 128/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0767 - acc: 0.9738 - val_loss: 0.0038 - val_acc: 0.9999\n",
            "Epoch 129/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0783 - acc: 0.9718 - val_loss: 0.0045 - val_acc: 0.9997\n",
            "Epoch 130/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0746 - acc: 0.9742 - val_loss: 0.0052 - val_acc: 0.9999\n",
            "Epoch 131/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0747 - acc: 0.9739 - val_loss: 0.0029 - val_acc: 1.0000\n",
            "Epoch 132/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0747 - acc: 0.9750 - val_loss: 0.0034 - val_acc: 1.0000\n",
            "Epoch 133/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0729 - acc: 0.9741 - val_loss: 0.0027 - val_acc: 0.9999\n",
            "Epoch 134/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0781 - acc: 0.9740 - val_loss: 0.0032 - val_acc: 1.0000\n",
            "Epoch 135/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0724 - acc: 0.9746 - val_loss: 0.0034 - val_acc: 0.9998\n",
            "Epoch 136/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0737 - acc: 0.9743 - val_loss: 0.0028 - val_acc: 1.0000\n",
            "Epoch 137/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0723 - acc: 0.9750 - val_loss: 0.0036 - val_acc: 0.9999\n",
            "Epoch 138/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0746 - acc: 0.9739 - val_loss: 0.0026 - val_acc: 1.0000\n",
            "Epoch 139/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0716 - acc: 0.9752 - val_loss: 0.0033 - val_acc: 0.9997\n",
            "Epoch 140/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0753 - acc: 0.9737 - val_loss: 0.0027 - val_acc: 1.0000\n",
            "Epoch 141/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0722 - acc: 0.9743 - val_loss: 0.0033 - val_acc: 0.9999\n",
            "Epoch 142/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0735 - acc: 0.9743 - val_loss: 0.0028 - val_acc: 1.0000\n",
            "Epoch 143/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0737 - acc: 0.9753 - val_loss: 0.0034 - val_acc: 0.9997\n",
            "Epoch 144/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0757 - acc: 0.9746 - val_loss: 0.0023 - val_acc: 1.0000\n",
            "Epoch 145/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0722 - acc: 0.9750 - val_loss: 0.0031 - val_acc: 0.9998\n",
            "Epoch 146/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0751 - acc: 0.9740 - val_loss: 0.0028 - val_acc: 0.9999\n",
            "Epoch 147/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0786 - acc: 0.9722 - val_loss: 0.0028 - val_acc: 1.0000\n",
            "Epoch 148/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0735 - acc: 0.9741 - val_loss: 0.0028 - val_acc: 1.0000\n",
            "Epoch 149/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0722 - acc: 0.9753 - val_loss: 0.0029 - val_acc: 1.0000\n",
            "Epoch 150/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0709 - acc: 0.9749 - val_loss: 0.0031 - val_acc: 1.0000\n",
            "Epoch 151/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0715 - acc: 0.9754 - val_loss: 0.0021 - val_acc: 1.0000\n",
            "Epoch 152/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0701 - acc: 0.9755 - val_loss: 0.0037 - val_acc: 0.9999\n",
            "Epoch 153/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0710 - acc: 0.9749 - val_loss: 0.0023 - val_acc: 0.9999\n",
            "Epoch 154/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0725 - acc: 0.9753 - val_loss: 0.0022 - val_acc: 1.0000\n",
            "Epoch 155/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0733 - acc: 0.9736 - val_loss: 0.0023 - val_acc: 0.9999\n",
            "Epoch 156/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0685 - acc: 0.9762 - val_loss: 0.0031 - val_acc: 1.0000\n",
            "Epoch 157/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0721 - acc: 0.9752 - val_loss: 0.0024 - val_acc: 1.0000\n",
            "Epoch 158/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0701 - acc: 0.9756 - val_loss: 0.0032 - val_acc: 0.9997\n",
            "Epoch 159/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0717 - acc: 0.9759 - val_loss: 0.0024 - val_acc: 0.9999\n",
            "Epoch 160/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0737 - acc: 0.9747 - val_loss: 0.0026 - val_acc: 0.9999\n",
            "Epoch 161/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0667 - acc: 0.9769 - val_loss: 0.0022 - val_acc: 1.0000\n",
            "Epoch 162/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0695 - acc: 0.9755 - val_loss: 0.0021 - val_acc: 0.9999\n",
            "Epoch 163/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0646 - acc: 0.9777 - val_loss: 0.0019 - val_acc: 1.0000\n",
            "Epoch 164/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0692 - acc: 0.9773 - val_loss: 0.0019 - val_acc: 1.0000\n",
            "Epoch 165/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0713 - acc: 0.9750 - val_loss: 0.0022 - val_acc: 1.0000\n",
            "Epoch 166/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0657 - acc: 0.9771 - val_loss: 0.0015 - val_acc: 1.0000\n",
            "Epoch 167/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0687 - acc: 0.9764 - val_loss: 0.0016 - val_acc: 1.0000\n",
            "Epoch 168/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0663 - acc: 0.9771 - val_loss: 0.0024 - val_acc: 0.9998\n",
            "Epoch 169/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0670 - acc: 0.9763 - val_loss: 0.0021 - val_acc: 0.9999\n",
            "Epoch 170/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0667 - acc: 0.9766 - val_loss: 0.0028 - val_acc: 0.9999\n",
            "Epoch 171/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0675 - acc: 0.9769 - val_loss: 0.0029 - val_acc: 0.9997\n",
            "Epoch 172/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0656 - acc: 0.9776 - val_loss: 0.0019 - val_acc: 1.0000\n",
            "Epoch 173/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0639 - acc: 0.9778 - val_loss: 0.0017 - val_acc: 1.0000\n",
            "Epoch 174/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0642 - acc: 0.9771 - val_loss: 0.0019 - val_acc: 1.0000\n",
            "Epoch 175/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0657 - acc: 0.9761 - val_loss: 0.0021 - val_acc: 1.0000\n",
            "Epoch 176/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0641 - acc: 0.9773 - val_loss: 0.0039 - val_acc: 0.9994\n",
            "Epoch 177/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0666 - acc: 0.9777 - val_loss: 0.0018 - val_acc: 1.0000\n",
            "Epoch 178/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0582 - acc: 0.9795 - val_loss: 0.0019 - val_acc: 1.0000\n",
            "Epoch 179/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0692 - acc: 0.9773 - val_loss: 0.0023 - val_acc: 1.0000\n",
            "Epoch 180/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0613 - acc: 0.9790 - val_loss: 0.0017 - val_acc: 1.0000\n",
            "Epoch 181/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0649 - acc: 0.9778 - val_loss: 0.0014 - val_acc: 1.0000\n",
            "Epoch 182/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0615 - acc: 0.9778 - val_loss: 0.0017 - val_acc: 0.9998\n",
            "Epoch 183/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0690 - acc: 0.9758 - val_loss: 0.0016 - val_acc: 0.9999\n",
            "Epoch 184/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0623 - acc: 0.9784 - val_loss: 0.0012 - val_acc: 1.0000\n",
            "Epoch 185/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0622 - acc: 0.9779 - val_loss: 0.0012 - val_acc: 1.0000\n",
            "Epoch 186/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0602 - acc: 0.9797 - val_loss: 0.0018 - val_acc: 1.0000\n",
            "Epoch 187/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0631 - acc: 0.9776 - val_loss: 0.0016 - val_acc: 1.0000\n",
            "Epoch 188/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0688 - acc: 0.9754 - val_loss: 0.0017 - val_acc: 0.9999\n",
            "Epoch 189/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0668 - acc: 0.9776 - val_loss: 0.0017 - val_acc: 1.0000\n",
            "Epoch 190/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0630 - acc: 0.9778 - val_loss: 0.0017 - val_acc: 0.9998\n",
            "Epoch 191/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0649 - acc: 0.9775 - val_loss: 0.0014 - val_acc: 1.0000\n",
            "Epoch 192/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0624 - acc: 0.9793 - val_loss: 0.0032 - val_acc: 0.9997\n",
            "Epoch 193/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0661 - acc: 0.9773 - val_loss: 0.0014 - val_acc: 1.0000\n",
            "Epoch 194/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0622 - acc: 0.9782 - val_loss: 0.0015 - val_acc: 1.0000\n",
            "Epoch 195/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0611 - acc: 0.9788 - val_loss: 0.0012 - val_acc: 1.0000\n",
            "Epoch 196/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0657 - acc: 0.9763 - val_loss: 0.0014 - val_acc: 1.0000\n",
            "Epoch 197/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0643 - acc: 0.9782 - val_loss: 0.0014 - val_acc: 1.0000\n",
            "Epoch 198/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0603 - acc: 0.9796 - val_loss: 0.0013 - val_acc: 1.0000\n",
            "Epoch 199/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0626 - acc: 0.9783 - val_loss: 0.0012 - val_acc: 1.0000\n",
            "Epoch 200/200\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0623 - acc: 0.9787 - val_loss: 9.8626e-04 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM-U14kCas-6"
      },
      "source": [
        "### 3.2. Evaluate the model on the test set\n",
        "\n",
        "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCYYqZHyas-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb91e4df-2c75-4043-e526-d71d88d50a6b"
      },
      "source": [
        "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
        "print('loss = ' + str(loss_and_acc[0]))\n",
        "print('accuracy = ' + str(loss_and_acc[1]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.7566 - acc: 0.8399\n",
            "loss = 0.7565870881080627\n",
            "accuracy = 0.839900016784668\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}