{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS584 Assignment 3 David Fu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Packages and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/jig728/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/jig728/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/jig728/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/jig728/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/jig728/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/jig728/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/jig728/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/jig728/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/jig728/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/jig728/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/jig728/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/jig728/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import nltk\n",
    "from nltk import word_tokenize, KneserNeyProbDist, SimpleGoodTuringProbDist, FreqDist, trigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.lm import Vocabulary, MLE\n",
    "from nltk.lm.models import KneserNeyInterpolated,Lidstone\n",
    "from nltk.lm.smoothing import KneserNey, WittenBell\n",
    "from nltk.lm.api import Smoothing\n",
    "from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, LSTM, GRU, Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.backend as backend\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Basic Reusable Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "READS Text File and return all of body as a string\n",
    "\"\"\"\n",
    "def read_input(input_path:str) -> str:\n",
    "    file_data = open(input_path , 'r')\n",
    "    return file_data.read()\n",
    "\n",
    "\"\"\"\n",
    "Use regex to remove punctuation, numbers and multi spaces\n",
    "\"\"\"\n",
    "def clean_text(text:str):\n",
    "    clean = re.sub('<unk>', '', text)\n",
    "    clean = re.sub('N', '', clean)\n",
    "    return re.sub(' +', ' ', clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build Tokenizer and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read text file and convert to token and vocabulary\n",
    "\"\"\"\n",
    "train_text = clean_text(read_input('./a3-data/train.txt'))\n",
    "valid_text = clean_text(read_input('./a3-data/valid.txt'))\n",
    "\n",
    "train_input = clean_text(train_text)\n",
    "valid_input = clean_text(valid_text)\n",
    "train_tokens = nltk.word_tokenize(train_input)\n",
    "valid_tokens = nltk.word_tokenize(valid_input)\n",
    "\n",
    "train_vocab = Vocabulary(train_tokens, unk_cutoff=5)\n",
    "valid_vocab = Vocabulary(valid_tokens, unk_cutoff=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9851\n",
      "1883\n"
     ]
    }
   ],
   "source": [
    "print(len(train_vocab))\n",
    "print(len(valid_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Regular expression to find sentenses and convert them to sets of 3 words for trigram model\n",
    "\"\"\"\n",
    "sentence_regex = lambda x: re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', x)\n",
    "train_token = [list(map(str.lower, word_tokenize(sentence))) for sentence in sentence_regex(train_text)]\n",
    "valid_token = [list(map(str.lower, word_tokenize(sentence))) for sentence in sentence_regex(valid_text)]\n",
    "\n",
    "train, vocab_train = padded_everygram_pipeline(3, train_token)\n",
    "test, vocab_test = padded_everygram_pipeline(3, valid_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build Trigram N-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Declar the gram model of length 3 and check the vocabuary list\n",
    "\"\"\"\n",
    "n3gram = MLE(3)\n",
    "len(n3gram.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9959"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fit the model to the training dataset with vacabulary and trainning data\n",
    "\"\"\"\n",
    "n3gram.fit(train, vocab_train)\n",
    "len(n3gram.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 3 ngram orders and 2573544 ngrams>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Check basic result of the trigram model by count of ngrams\n",
    "\"\"\"\n",
    "print(n3gram.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03286384976525822\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pull a few example trigram from the Text hard coded to see if the probability is present\n",
    "\"\"\"\n",
    "print(n3gram.score('post', 'washington'.split()))\n",
    "\n",
    "print(n3gram.score('institute', 'national cancer'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Good Turing smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SimpleGoodTuringProbDist based on 4766534 samples>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Using trigram model with frequency distribution to simple Turning Distirbution for normalization\n",
    "\"\"\"\n",
    "ngrams = trigrams(train_text)\n",
    "freq_dist = FreqDist(ngrams)\n",
    "turing = SimpleGoodTuringProbDist(freq_dist)\n",
    "\n",
    "print(turing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KneserNeyProbDist based on 4766534 trigrams\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create a KnesnerNeyDistribution\n",
    "\"\"\"\n",
    "kneser = KneserNeyProbDist(freq_dist)\n",
    "print(kneser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model it to Trigram with the input data set and vocabulary\n",
    "\"\"\"\n",
    "kn = KneserNeyInterpolated(3)\n",
    "kn.fit(train, vocab_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set up a basic model to test sliding window of the trigram with sets of 30 from the validation data\n",
    "\"\"\"\n",
    "\n",
    "windows = []\n",
    "for i in range(29, len(valid_tokens)):\n",
    "    window = valid_tokens[i-29: i+1]\n",
    "    windows.append(window)\n",
    "previous_29 = [i[:-1] for i in windows]\n",
    "actual_30 = [''.join(i[-1:]) for i in windows]\n",
    "\n",
    "predict_word = [n3gram.generate(1,text_seed=i, random_seed=13) for i in previous_29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predict word: \n",
      "['nine', 'her', 'now', 'locations', 'houses', 'five', 'cents', 'feet', 'ago', 'citicorp', 'call', 'document', 'economic', 'broadcast', 'electronics', 'in', 'early', 'for', 'recession', 'of', 'contributions', 'as', 'access', 'projects', 'for', 'is', 'dollar', 'guard', 'program', 'in']\n",
      "\n",
      "The actual word in test set:\n",
      "['years', 'from', 'among', 'four', 'or', 'five', 'two', 'weeks', 'ago', 'viewers', 'of', 'several', 'nbc', 'consumer', 'segments', 'started', 'calling', 'a', 'number', 'for', 'advice', 'on', 'various', 'issues', 'and', 'the', 'new', 'syndicated', 'reality', 'show']\n"
     ]
    }
   ],
   "source": [
    "print('The predict word: ')\n",
    "print(predict_word[:30])\n",
    "print('\\nThe actual word in test set:')\n",
    "print(actual_30[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perplexity and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Predicting the next word in sliding window : 0.108417\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(predict_word)):\n",
    "    if actual_30[i] == predict_word[i]:\n",
    "        correct += 1\n",
    "print('The accuracy of Predicting the next word in sliding window : %f' % (correct/len(predict_word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity scores of ngram model on test set is :10031.680732\n"
     ]
    }
   ],
   "source": [
    "perplexity = np.array([n3gram.perplexity(i) for i in valid_tokens])\n",
    "perplexity_ngram = np.ma.masked_invalid(perplexity).mean()\n",
    "print('The perplexity scores of ngram model on test set is :%f' % perplexity_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 30 line prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"but while the new york stock exchange did n't fall ___\",\n",
       " 'some circuit breakers installed after the october N crash failed ___',\n",
       " 'the N stock specialist firms on the big board floor ___',\n",
       " 'big investment banks refused to step up to the plate ___',\n",
       " \"heavy selling of standard & poor 's 500-stock index futures ___\",\n",
       " 'seven big board stocks ual amr bankamerica walt disney capital ___',\n",
       " 'once again the specialists were not able to handle the ___',\n",
       " '<unk> james <unk> chairman of specialists henderson brothers inc. it ___',\n",
       " 'when the dollar is in a <unk> even central banks ___',\n",
       " 'speculators are calling for a degree of liquidity that is ___',\n",
       " 'many money managers and some traders had already left their ___',\n",
       " 'then in a <unk> plunge the dow jones industrials in ___',\n",
       " '<unk> trading accelerated to N million shares a record for ___',\n",
       " 'at the end of the day N million shares were ___',\n",
       " \"the dow 's decline was second in point terms only ___\",\n",
       " \"in percentage terms however the dow 's dive was the ___\",\n",
       " 'shares of ual the parent of united airlines were extremely ___',\n",
       " \"wall street 's takeover-stock speculators or risk arbitragers had placed ___\",\n",
       " 'at N p.m. edt came the <unk> news the big ___',\n",
       " 'on the exchange floor as soon as ual stopped trading ___',\n",
       " 'several traders could be seen shaking their heads when the ___',\n",
       " 'for weeks the market had been nervous about takeovers after ___',\n",
       " 'and N minutes after the ual trading halt came news ___',\n",
       " \"arbitragers could n't dump their ual stock but they rid ___\",\n",
       " 'for example their selling caused trading halts to be declared ___',\n",
       " 'but as panic spread speculators began to sell blue-chip stocks ___',\n",
       " 'when trading was halted in philip morris the stock was ___',\n",
       " 'selling <unk> because of waves of automatic stop-loss orders which ___',\n",
       " 'most of the stock selling pressure came from wall street ___',\n",
       " 'traders said most of their major institutional investors on the ___']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = read_input('./a3-data/input.txt')\n",
    "input_sent = input_text.split('\\n')\n",
    "first_30 = input_sent[0:30]\n",
    "first_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "['but', 'while', 'the', 'new', 'york', 'stock', 'exchange', 'did', \"n't\", 'fall']\n",
      "but while the new york stock exchange did n't fall ___  predict: the\n",
      "\n",
      "Example 2:\n",
      "['some', 'circuit', 'breakers', 'installed', 'after', 'the', 'october', 'N', 'crash', 'failed']\n",
      "some circuit breakers installed after the october N crash failed ___  predict: to\n",
      "\n",
      "Example 3:\n",
      "['the', 'N', 'stock', 'specialist', 'firms', 'on', 'the', 'big', 'board', 'floor']\n",
      "the N stock specialist firms on the big board floor ___  predict: at\n",
      "\n",
      "Example 4:\n",
      "['big', 'investment', 'banks', 'refused', 'to', 'step', 'up', 'to', 'the', 'plate']\n",
      "big investment banks refused to step up to the plate ___  predict: is\n",
      "\n",
      "Example 5:\n",
      "['heavy', 'selling', 'of', 'standard', '&', 'poor', \"'s\", '500-stock', 'index', 'futures']\n",
      "heavy selling of standard & poor 's 500-stock index futures ___  predict: is\n",
      "\n",
      "Example 6:\n",
      "['seven', 'big', 'board', 'stocks', 'ual', 'amr', 'bankamerica', 'walt', 'disney', 'capital']\n",
      "seven big board stocks ual amr bankamerica walt disney capital ___  predict: international\n",
      "\n",
      "Example 7:\n",
      "['once', 'again', 'the', 'specialists', 'were', 'not', 'able', 'to', 'handle', 'the']\n",
      "once again the specialists were not able to handle the ___  predict: earthquake-related\n",
      "\n",
      "Example 8:\n",
      "['<unk>', 'james', '<unk>', 'chairman', 'of', 'specialists', 'henderson', 'brothers', 'inc.', 'it']\n",
      "<unk> james <unk> chairman of specialists henderson brothers inc. it ___  predict: is\n",
      "\n",
      "Example 9:\n",
      "['when', 'the', 'dollar', 'is', 'in', 'a', '<unk>', 'even', 'central', 'banks']\n",
      "when the dollar is in a <unk> even central banks ___  predict: of\n",
      "\n",
      "Example 10:\n",
      "['speculators', 'are', 'calling', 'for', 'a', 'degree', 'of', 'liquidity', 'that', 'is']\n",
      "speculators are calling for a degree of liquidity that is ___  predict: larger\n",
      "\n",
      "Example 11:\n",
      "['many', 'money', 'managers', 'and', 'some', 'traders', 'had', 'already', 'left', 'their']\n",
      "many money managers and some traders had already left their ___  predict: last\n",
      "\n",
      "Example 12:\n",
      "['then', 'in', 'a', '<unk>', 'plunge', 'the', 'dow', 'jones', 'industrials', 'in']\n",
      "then in a <unk> plunge the dow jones industrials in ___  predict: new\n",
      "\n",
      "Example 13:\n",
      "['<unk>', 'trading', 'accelerated', 'to', 'N', 'million', 'shares', 'a', 'record', 'for']\n",
      "<unk> trading accelerated to N million shares a record for ___  predict: elections\n",
      "\n",
      "Example 14:\n",
      "['at', 'the', 'end', 'of', 'the', 'day', 'N', 'million', 'shares', 'were']\n",
      "at the end of the day N million shares were ___  predict: outstanding\n",
      "\n",
      "Example 15:\n",
      "['the', 'dow', \"'s\", 'decline', 'was', 'second', 'in', 'point', 'terms', 'only']\n",
      "the dow 's decline was second in point terms only ___  predict: limited\n",
      "\n",
      "Example 16:\n",
      "['in', 'percentage', 'terms', 'however', 'the', 'dow', \"'s\", 'dive', 'was', 'the']\n",
      "in percentage terms however the dow 's dive was the ___  predict: lead\n",
      "\n",
      "Example 17:\n",
      "['shares', 'of', 'ual', 'the', 'parent', 'of', 'united', 'airlines', 'were', 'extremely']\n",
      "shares of ual the parent of united airlines were extremely ___  predict: pleased\n",
      "\n",
      "Example 18:\n",
      "['wall', 'street', \"'s\", 'takeover-stock', 'speculators', 'or', 'risk', 'arbitragers', 'had', 'placed']\n",
      "wall street 's takeover-stock speculators or risk arbitragers had placed ___  predict: directly\n",
      "\n",
      "Example 19:\n",
      "['at', 'N', 'p.m.', 'edt', 'came', 'the', '<unk>', 'news', 'the', 'big']\n",
      "at N p.m. edt came the <unk> news the big ___  predict: board\n",
      "\n",
      "Example 20:\n",
      "['on', 'the', 'exchange', 'floor', 'as', 'soon', 'as', 'ual', 'stopped', 'trading']\n",
      "on the exchange floor as soon as ual stopped trading ___  predict: altogether\n",
      "\n",
      "Example 21:\n",
      "['several', 'traders', 'could', 'be', 'seen', 'shaking', 'their', 'heads', 'when', 'the']\n",
      "several traders could be seen shaking their heads when the ___  predict: issuing\n",
      "\n",
      "Example 22:\n",
      "['for', 'weeks', 'the', 'market', 'had', 'been', 'nervous', 'about', 'takeovers', 'after']\n",
      "for weeks the market had been nervous about takeovers after ___  predict: ms\n",
      "\n",
      "Example 23:\n",
      "['and', 'N', 'minutes', 'after', 'the', 'ual', 'trading', 'halt', 'came', 'news']\n",
      "and N minutes after the ual trading halt came news ___  predict: of\n",
      "\n",
      "Example 24:\n",
      "['arbitragers', 'could', \"n't\", 'dump', 'their', 'ual', 'stock', 'but', 'they', 'rid']\n",
      "arbitragers could n't dump their ual stock but they rid ___  predict: of\n",
      "\n",
      "Example 25:\n",
      "['for', 'example', 'their', 'selling', 'caused', 'trading', 'halts', 'to', 'be', 'declared']\n",
      "for example their selling caused trading halts to be declared ___  predict: in\n",
      "\n",
      "Example 26:\n",
      "['but', 'as', 'panic', 'spread', 'speculators', 'began', 'to', 'sell', 'blue-chip', 'stocks']\n",
      "but as panic spread speculators began to sell blue-chip stocks ___  predict: posted\n",
      "\n",
      "Example 27:\n",
      "['when', 'trading', 'was', 'halted', 'in', 'philip', 'morris', 'the', 'stock', 'was']\n",
      "when trading was halted in philip morris the stock was ___  predict: dragged\n",
      "\n",
      "Example 28:\n",
      "['selling', '<unk>', 'because', 'of', 'waves', 'of', 'automatic', 'stop-loss', 'orders', 'which']\n",
      "selling <unk> because of waves of automatic stop-loss orders which ___  predict: are\n",
      "\n",
      "Example 29:\n",
      "['most', 'of', 'the', 'stock', 'selling', 'pressure', 'came', 'from', 'wall', 'street']\n",
      "most of the stock selling pressure came from wall street ___  predict: is\n",
      "\n",
      "Example 30:\n",
      "['traders', 'said', 'most', 'of', 'their', 'major', 'institutional', 'investors', 'on', 'the']\n",
      "traders said most of their major institutional investors on the ___  predict: line\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Use model to predict the 30 lines in the sample input sentenses\n",
    "\"\"\"\n",
    "for i in range(30):\n",
    "    print('\\nExample %d:' % (i+1))\n",
    "    token = first_30[i][:-3].split()\n",
    "    print(token)\n",
    "    predict_word = n3gram.generate(1,text_seed=token, random_seed=9)\n",
    "    result = first_30[i] + '  predict: ' + predict_word\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 9649\n",
      "Total windows: 810124\n",
      "Window size: 20\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Using the tokenizer to the training input and encode the words to a number setting window size to 20 for trainning data\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([train_input])\n",
    "encoded_word = tokenizer.texts_to_sequences([train_input])[0]\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "\n",
    "windows = []\n",
    "for i in range(19, len(encoded_word)):\n",
    "    window = encoded_word[i-19:i+1]\n",
    "    windows.append(window)\n",
    "print('Total windows: %d' % len(windows))\n",
    "\n",
    "max_length = max([len(seq) for seq in windows])\n",
    "windows = pad_sequences(windows, maxlen=max_length, padding='pre')\n",
    "print('Window size: %d' % max_length)\n",
    "\n",
    "X_train, y = windows[:,:-1],windows[:,-1]\n",
    "y_train = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 9649\n",
      "Total Windows: 64231\n",
      "Window size: 20\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Using the tokenizer to the valid input and encode the words to a number setting window size to 20 for validation data\n",
    "\"\"\"\n",
    "\n",
    "tokenizer.fit_on_texts([valid_input])\n",
    "encoded_2 = tokenizer.texts_to_sequences([valid_input])[0]\n",
    "\n",
    "vocab_size_2 = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size_2)\n",
    "\n",
    "windows2 = []\n",
    "for i in range(19, len(encoded_2)):\n",
    "    window2 = encoded_2[i-19:i+1]\n",
    "    windows2.append(window2)\n",
    "print('Total Windows: %d' % len(windows2))\n",
    "\n",
    "max_length2 = max([len(seq) for seq in windows2])\n",
    "windows2 = pad_sequences(windows2, maxlen=max_length, padding='pre')\n",
    "print('Window size: %d' % max_length)\n",
    "\n",
    "X_test, y = windows2[:,:-1],windows2[:,-1]\n",
    "y_test = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 19, 10)            96490     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 30)                3690      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 9649)              299119    \n",
      "=================================================================\n",
      "Total params: 399,299\n",
      "Trainable params: 399,299\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set up sequentail model setting vocab with LSTM to predict the word\n",
    "\"\"\"\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_length-1))\n",
    "model.add(GRU(30))\n",
    "model.add(Dense(vocab_size, activation='relu'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(predict, label):\n",
    "    return backend.exp(backend.categorical_crossentropy(predict, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jig728/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/jig728/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/10\n",
      "810124/810124 [==============================] - 760s 938us/step - loss: 11.3770 - perplexity: 6267494.0000\n",
      "Epoch 2/10\n",
      "810124/810124 [==============================] - 761s 939us/step - loss: 11.1680 - perplexity: 6112641.5000\n",
      "Epoch 3/10\n",
      "810124/810124 [==============================] - 1054s 1ms/step - loss: 11.1167 - perplexity: 6078264.5000\n",
      "Epoch 4/10\n",
      "810124/810124 [==============================] - 1074s 1ms/step - loss: 11.0657 - perplexity: 6040219.5000\n",
      "Epoch 5/10\n",
      "810124/810124 [==============================] - 940s 1ms/step - loss: 11.0523 - perplexity: 6033553.5000\n",
      "Epoch 6/10\n",
      "810124/810124 [==============================] - 1047s 1ms/step - loss: 11.0315 - perplexity: 6014550.0000\n",
      "Epoch 7/10\n",
      "810124/810124 [==============================] - 979s 1ms/step - loss: 11.0312 - perplexity: 6018562.5000\n",
      "Epoch 8/10\n",
      "810124/810124 [==============================] - 885s 1ms/step - loss: 11.0560 - perplexity: 6043589.0000\n",
      "Epoch 9/10\n",
      "810124/810124 [==============================] - 998s 1ms/step - loss: 11.0412 - perplexity: 6034362.5000\n",
      "Epoch 10/10\n",
      "810124/810124 [==============================] - 1036s 1ms/step - loss: 11.0272 - perplexity: 6019807.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb9b5fd6750>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set up model for calculation with bath size 50 and epochs 100\n",
    "\"\"\"\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.1), metrics=[perplexity])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write the model to file to be reused\n",
    "\"\"\"\n",
    "\n",
    "model_output = model.to_json()\n",
    "with open(\"model20.json\", \"w\") as json_file:\n",
    "    json_file.write(model_output)\n",
    "\n",
    "model.save_weights(\"model20.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jig728/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py:819: UserWarning: Output dense_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_1.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model cannot be compiled because it has no loss to optimize.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b5cd16e15a91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model50.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m#                   loss_weight_2 * output_2_loss_fn(...) +\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;31m#                   layer losses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_total_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# Functions for train, test and predict will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_prepare_total_loss\u001b[0;34m(self, masks)\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m                     raise ValueError('The model cannot be compiled '\n\u001b[0m\u001b[1;32m    707\u001b[0m                                      'because it has no loss to optimize.')\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The model cannot be compiled because it has no loss to optimize."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For rapid reload instead of rerunning everytime\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model_file = open('model50.json', 'r')\n",
    "loaded_model = model_file.read()\n",
    "model_file.close()\n",
    "model = model_from_json(loaded_model)\n",
    "model.compile(optimizer=Adam(learning_rate=0.1))\n",
    "model.load_weights(\"model50.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Error when checking model target: expected no data, but got:', array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-b98a612d4af8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mloss_and_per\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_and_per\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mperplexity_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_and_per\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1347\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1350\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     68\u001b[0m             raise ValueError('Error when checking model ' +\n\u001b[1;32m     69\u001b[0m                              \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                              'expected no data, but got:', data)\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('Error when checking model target: expected no data, but got:', array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Check the model and calculate the perplexity\n",
    "\"\"\"\n",
    "\n",
    "loss_and_per = model.evaluate(X_test, y_test)\n",
    "loss = loss_and_per[0]\n",
    "perplexity_test = loss_and_per[1]\n",
    "print('loss = ' + str(loss))\n",
    "print('perplexity = ' + str(loss_and_per[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using given value for proof\n",
    "\"\"\"\n",
    "sample_size = 64231\n",
    "batch_size = 50\n",
    "number_of_predictions = 1284\n",
    "\n",
    "total_loss = loss*sample_size/batch_size\n",
    "aim = np.exp(total_loss/number_of_predictions)\n",
    "print(aim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use RNN for prediction\n",
    "\"\"\"\n",
    "def rnn_prediction(text):\n",
    "    encoded = tokenizer.texts_to_sequences([text])[0]\n",
    "    \n",
    "    encoded = pad_sequences([encoded], maxlen=max_length-1, padding='pre')\n",
    "    \n",
    "    y_predict_class = model.predict_classes(encoded, verbose=0)\n",
    "   \n",
    "    predict_word = ''\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == y_predict_class:\n",
    "            predict_word = word\n",
    "            break\n",
    "    \n",
    "    result = text.rstrip() + ' prediction: ' + predict_word\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Print the result of the 30 predictions\n",
    "\"\"\"\n",
    "for i in range(30):\n",
    "    text = first_30[i]\n",
    "    print('\\nExample ' + str(i+1) + ':')\n",
    "    rnn_prediction(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
